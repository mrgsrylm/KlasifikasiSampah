# -*- coding: utf-8 -*-
"""klasifikasi_sampah_deteksi_produk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KehgrG9U44FrQPDB44_EPoDlipN4SVb8

# **Klasifikasi Sampah Deteksi Produk dengan Convolutional Neural Network (CNN)**
---
"""

# File operations and data manipulation
import os, zipfile, shutil
from google.colab import drive
from shutil import copyfile

# Random number generation and numerical computations
import random
import numpy as np

# Visualization library
import matplotlib.pyplot as plt

# Deep learning framework and its modules
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator

# Ignore warnings during execution
import warnings
warnings.filterwarnings("ignore")

# Print TensorFlow version
print(tf.__version__)

"""## **Data Collection**
---
"""

!wget --no-check-certificate \
  https://github.com/mrgsrylm/KlasifikasiSampah/releases/download/v1.0.0/komposisi_sampah.zip\
  -O komposisi_sampah.zip

!unzip komposisi_sampah.zip

"""## **Data Pre-Processing**
---

### Settings Hyperparameter
---
"""

# Constants for data splitting (T:90,V:10), batch size, input size (image dimensions), number of epochs, and data sample size
SPLIT_SIZE = 0.9
BATCH_SIZE = 32
INPUT_SIZE = (150, 150)
EPOCH = 20
N = 16

"""### Training & Validation Directories
---
"""

def create_directory(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)

def create_category_dirs(root_path, name, subs):
    sub_dir = os.path.join(root_path, name)

    # Create all directories
    create_directory(sub_dir)
    for category in subs:
        create_directory(os.path.join(sub_dir, category))

DATASET_DIR = "dataset"
try:
    CATEGORIES_DIR = ["baterai", "botol_kaca", "botol_plastik",
                      "kaleng", "kardus", "kertas",
                      "plastik", "sterefoam"]

    create_category_dirs(DATASET_DIR, "TRAINING", CATEGORIES_DIR)
    create_category_dirs(DATASET_DIR, "VALIDATION", CATEGORIES_DIR)
    print("Successful!")
except FileExistsError:
    print("You should not be seeing this since the upper directory is removed beforehand")

for rootdir, dirs, files in os.walk(DATASET_DIR):
    for subdir in dirs:
        print(os.path.join(rootdir, subdir))

"""### Splitting Dataset
---
"""

# split_data function: This function splits data from the source directory into training and validation sets
def split_data(SOURCE, TRAIN, VALIDATION, SPLIT_SIZE):
    # Get a list of non-empty files in the source director
    files = [filename for filename in os.listdir(SOURCE) if os.path.getsize(os.path.join(SOURCE, filename)) > 0]

    # Shuffle the list of files
    shuffle_set = random.sample(files, len(files))

    # Calculate the number of files for training and validation sets based on the split size
    training_len = int(len(files) * SPLIT_SIZE)
    validation_len = len(files) - training_len

    # Divide the shuffled files into training and validation sets
    training_set = shuffle_set[:training_len]
    validation_set = shuffle_set[-validation_len:]

    for filename in training_set:
        source = os.path.join(SOURCE, filename)
        destiny = os.path.join(TRAIN, filename)
        copyfile(source, destiny)

    for filename in validation_set:
        source = os.path.join(SOURCE, filename)
        destiny = os.path.join(VALIDATION, filename)
        copyfile(source, destiny)

DATASOURCE = "komposisi_sampah"
BATERAI_DATASOURCE = os.path.join(DATASOURCE, 'baterai')
BOTOL_KACA_DATASOURCE = os.path.join(DATASOURCE, "botol_kaca")
BOTOL_PLASTIK_DATASOURCE = os.path.join(DATASOURCE, "botol_plastik")
KALENG_DATASOURCE = os.path.join(DATASOURCE, "kaleng")
KARDUS_DATASOURCE = os.path.join(DATASOURCE, "kardus")
KERTAS_DATASOURCE = os.path.join(DATASOURCE, "kertas")
PLASTIK_DATASOURCE = os.path.join(DATASOURCE, "plastik")
STEREFOAM_DATASOURCE = os.path.join(DATASOURCE, "sterefoam")

TRAINING_DATASET = "dataset/TRAINING"
BATERAI_TRAINING_DASASET = os.path.join(TRAINING_DATASET, 'baterai')
BOTOL_KACA_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "botol_kaca")
BOTOL_PLASTIK_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "botol_plastik")
KALENG_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "kaleng")
KARDUS_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "kardus")
KERTAS_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "kertas")
PLASTIK_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "plastik")
STEREFOAM_TRAINING_DATASET = os.path.join(TRAINING_DATASET, "sterefoam")

VALIDATION_DATASET = "dataset/VALIDATION"
BATERAI_VALIDATION_DASASET = os.path.join(VALIDATION_DATASET, 'baterai')
BOTOL_KACA_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "botol_kaca")
BOTOL_PLASTIK_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "botol_plastik")
KALENG_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "kaleng")
KARDUS_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "kardus")
KERTAS_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "kertas")
PLASTIK_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "plastik")
STEREFOAM_VALIDATION_DATASET = os.path.join(VALIDATION_DATASET, "sterefoam")

# Clear existing files
category_directories = [
  BATERAI_TRAINING_DASASET, BATERAI_VALIDATION_DASASET,
  BOTOL_KACA_TRAINING_DATASET, BOTOL_KACA_VALIDATION_DATASET,
  BOTOL_PLASTIK_TRAINING_DATASET, BOTOL_PLASTIK_VALIDATION_DATASET,
  KALENG_TRAINING_DATASET, KALENG_VALIDATION_DATASET,
  KARDUS_TRAINING_DATASET, KARDUS_VALIDATION_DATASET,
  KERTAS_TRAINING_DATASET, KERTAS_VALIDATION_DATASET,
  PLASTIK_TRAINING_DATASET, PLASTIK_VALIDATION_DATASET,
  STEREFOAM_TRAINING_DATASET, STEREFOAM_VALIDATION_DATASET

]
for directory in category_directories:
    for file in os.scandir(directory):
        if file.is_file():
            os.remove(file.path)

# Do splitting and move data
split_data(BATERAI_DATASOURCE, BATERAI_TRAINING_DASASET, BATERAI_VALIDATION_DASASET, SPLIT_SIZE)
split_data(BOTOL_KACA_DATASOURCE, BOTOL_KACA_TRAINING_DATASET, BOTOL_KACA_VALIDATION_DATASET, SPLIT_SIZE)
split_data(BOTOL_PLASTIK_DATASOURCE, BOTOL_PLASTIK_TRAINING_DATASET, BOTOL_PLASTIK_VALIDATION_DATASET, SPLIT_SIZE)
split_data(KALENG_DATASOURCE, KALENG_TRAINING_DATASET, KALENG_VALIDATION_DATASET, SPLIT_SIZE)
split_data(KARDUS_DATASOURCE, KARDUS_TRAINING_DATASET, KARDUS_VALIDATION_DATASET, SPLIT_SIZE)
split_data(KERTAS_DATASOURCE, KERTAS_TRAINING_DATASET, KERTAS_VALIDATION_DATASET, SPLIT_SIZE)
split_data(PLASTIK_DATASOURCE, PLASTIK_TRAINING_DATASET, PLASTIK_VALIDATION_DATASET, SPLIT_SIZE)
split_data(STEREFOAM_DATASOURCE, STEREFOAM_TRAINING_DATASET, STEREFOAM_VALIDATION_DATASET, SPLIT_SIZE)

for source in category_directories:
    product_name = os.path.basename(source)
    print(f"There are {len(os.listdir(source))} {product_name} images.")

"""### Training Validation Generator and Data Augmentation
---
"""

# train_val_generators function: This function creates and configures data generators for training and validation data.
def train_val_generators(TRAINING_DATASET, VALIDATION_DATASET, BATCH_SIZE, INPUT_SIZE):

    # Training data augmentation configuration
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    # Validation data augmentation configuration
    validation_datagen = ImageDataGenerator(rescale=1./255)

    # Generating batches of augmented data for training
    train_generator = train_datagen.flow_from_directory(
        directory=TRAINING_DATASET,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        target_size=INPUT_SIZE
    )

    # Generating batches of non-augmented data for validation
    validation_generator = validation_datagen.flow_from_directory(
        directory=VALIDATION_DATASET,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        target_size=INPUT_SIZE
    )

    # Return the configured data generators for training and validation
    return train_generator, validation_generator

# TRAIN AND VALIDATION GENERATOR
TRAINING_GENERATOR, VALIDATION_GENERATOR = train_val_generators(TRAINING_DATASET, VALIDATION_DATASET, BATCH_SIZE, INPUT_SIZE)

"""## **Model Simple CNN**

### Model Definition
---

#### Create Model using CNN and Implement Flatten, Dropout, and Regularization
---
"""

# create_model function: This function defines and compiles a convolutional neural network model.
def create_model_simple_cnn(INPUT_SHAPE, NUM_CLASSES):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=INPUT_SHAPE),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    # Compile the model with appropriate optimizer, loss function, and metrics
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

# Create Model
INPUT_SHAPE = (150, 150, 3)
NUM_CLASSES = 8
model_simple_cnn = create_model_simple_cnn(INPUT_SHAPE, NUM_CLASSES)
model_simple_cnn.summary()

"""### **Training**
---
"""

# Training the model using the training generator and validating it using the validation generator
# 'EPOCH' specifies the number of epochs for training
# The 'verbose' parameter is set to 1 for detailed progress output during training
history_simple_cnn = model_simple_cnn.fit(TRAINING_GENERATOR,
                                          epochs=EPOCH,
                                          validation_data=VALIDATION_GENERATOR,
                                          verbose=1)

"""### **Evaluation**
---

#### Plotting Accuracy and Model Evaluation
---
"""

acc = history_simple_cnn.history['accuracy']
val_acc = history_simple_cnn.history['val_accuracy']
loss = history_simple_cnn.history['loss']
val_loss = history_simple_cnn.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

# Evaluating the model on the validation generator using the evaluate_generator method
# 'steps' parameter specifies the total number of steps (batches of samples) to yield from validation_generator before stopping
# 'verbose=0' means silent mode, no progress bar will be displayed during evaluation
_, accuracy = model_simple_cnn.evaluate_generator(VALIDATION_GENERATOR, steps=10, verbose=0)

# Printing the test accuracy after multiplying by 100 to convert it to a percentage
print('Test Accuracy: %.3f%%' % (accuracy * 100))

"""### **Test**
---
"""

# Getting a batch of test data and corresponding labels from the validation generator
test_x, test_y = VALIDATION_GENERATOR.__getitem__(1)

# Extracting class labels and creating a dictionary for label decoding
labels = VALIDATION_GENERATOR.class_indices
labels = dict((v, k) for k, v in labels.items())

# Making predictions using the model on the test data
y_pred = model_simple_cnn.predict(test_x)

# Plotting a grid of images along with their predicted and true labels
plt.figure(figsize=(16, 16))
n = 16  # Number of images to display
count = 0  # Counter for correctly predicted images
for i in range(n):
    plt.subplot(4, 4, i+1)
    plt.title('pred:%s / truth:%s' % (labels[np.argmax(y_pred[i])], labels[np.argmax(test_y[i])]))
    plt.imshow(test_x[i])
    if np.argmax(y_pred[i]) == np.argmax(test_y[i]):
        count += 1

# Calculating accuracy based on correct predictions
accuracy = count / n * 100

# Printing the accuracy
print('Accuracy: {}%'.format(accuracy))

"""## **Model Inception CNN**"""

from keras.applications.inception_v3 import InceptionV3
from keras import layers
from keras import Model

"""### Model Definition
---

#### Create Model Inception using Transfer Learning
---
"""

inception_pretrained_model = InceptionV3(input_shape=(150, 150, 3),
                                include_top=False,  # Excluding the top (classification) layers
                                weights='imagenet')  # Using pre-trained weights from ImageNet

for layer in inception_pretrained_model.layers:
    layer.trainable = False

inception_pretrained_model.summary()

# Getting the output from a specific layer ('mixed6') of the pre-trained InceptionV3 model
last_layer = inception_pretrained_model.get_layer('mixed6')
print('Last layer output shape:', last_layer.output_shape)  # Printing the output shape of the selected layer
last_output = last_layer.output  # Storing the output tensor of the selected layer for further processing

# Adding a Flatten layer to convert the 3D tensor to 1D vector
x = layers.Flatten()(last_output)

# Adding a Dense layer with 512 units and ReLU activation function
x = layers.Dense(512, activation='relu')(x)

# Applying Dropout with a dropout rate of 0.2 to prevent overfitting
x = layers.Dropout(0.2)(x)

# Adding the final Dense layer with 2 units (for binary classification) and sigmoid activation function
predictions = layers.Dense(8, activation='softmax')(x)

# Creating the custom model by specifying its input (pretrained InceptionV3 model's input) and output (predictions)
model_inception = Model(inception_pretrained_model.input, predictions)

# Compiling the InceptionV3-based model
model_inception.compile(optimizer=tf.keras.optimizers.Adam(0.0001),  # Using Adam optimizer with a learning rate of 0.0001
                        loss='categorical_crossentropy',  # Binary crossentropy loss for binary classification task
                        metrics=['accuracy'])  # Monitoring accuracy as the evaluation metric

# Printing the summary of the compiled model architecture
model_inception.summary()

"""### **Training**
---
"""

# Training the model using the training generator and validating it using the validation generator
history_inception = model_inception.fit(TRAINING_GENERATOR,  # Training data generator
                                        epochs=EPOCH,  # Number of epochs for training
                                        validation_data=VALIDATION_GENERATOR,  # Validation data generator
                                        verbose=1)  # Setting verbosity level to 1 for detailed progress output during training

"""### **Evaluation**
---

#### Plotting Accuracy and Model Evaluation
---
"""

acc = history_inception.history['accuracy']
val_acc = history_inception.history['val_accuracy']
loss = history_inception.history['loss']
val_loss = history_inception.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and Validation loss')
plt.legend()

plt.show()

_, acc = model_inception.evaluate_generator(VALIDATION_GENERATOR, steps=10, verbose=0)
print('Test Accuracy: %.3f ' % (acc * 100))

"""### **Test**
---
"""

est_x, test_y = VALIDATION_GENERATOR.__getitem__(1)

labels = (VALIDATION_GENERATOR.class_indices)
labels = dict((v,k) for k,v in labels.items())

y_pred = model_inception.predict(test_x)

plt.figure(figsize=(16, 16))
n = 16
count = 0

for i in range(n):
    plt.subplot(4, 4, i+1)
    plt.title('pred:%s / truth:%s' % (labels[np.argmax(y_pred[i])], labels[np.argmax(test_y[i])]))
    plt.imshow(test_x[i])
    if (np.argmax(y_pred[i]) == np.argmax(test_y[i])):
      count += 1

akurasi = count/n*100
print('Accuracy : {} %'.format(akurasi))

"""### **Store Model**
---

#### Save Model (h5)
---
"""

!mkdir -p "saved_model_inception"
!mkdir -p "tflite_model_inception"
model_inception.save('saved_model_inception/model_class_detect_inceptionv3.h5')

"""#### Convert to TFLite
---
"""

tf.saved_model.save(model_inception, export_dir="saved_model_inception")

converter = tf.lite.TFLiteConverter.from_saved_model("saved_model_inception")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model_inception = converter.convert()

import pathlib
tflite_model_file = pathlib.Path("tflite_model_inception/model_class_detect_inveptionv3.tflite")
tflite_model_file.write_bytes(tflite_model_inception)

"""## **Model Xception CNN**
---
"""

from keras.applications.xception import Xception
from keras import layers
from keras import Model

"""### **Model Definition**
---

#### Create Model using Transfer Learning
---
"""

xception_pretrained_model = Xception(input_shape = (150, 150, 3),
                                     include_top = False,
                                     weights = "imagenet")

for layer in xception_pretrained_model.layers:
  layer.trainable = False

xception_pretrained_model.summary()

last_layer = xception_pretrained_model.get_layer('add_11')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = layers.Flatten()(last_output)
x = layers.Dense(512, activation='relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(8, activation='softmax')(x)
model_xception = Model(xception_pretrained_model.input, x)

model_xception.compile(optimizer=tf.optimizers.Adam(0.0001),
                       loss="categorical_crossentropy",
                       metrics=["accuracy"])
model_xception.summary()

"""### **Training**
---
"""

history_xception = model_xception.fit(TRAINING_GENERATOR,
                                      epochs = EPOCH,
                                      validation_data = VALIDATION_GENERATOR,
                                      verbose = 1)

"""### **Evaluation**
---

#### Plotting Accuracy and Model Evaluation
---
"""

acc = history_xception.history['accuracy']
val_acc = history_xception.history['val_accuracy']
loss = history_xception.history['loss']
val_loss = history_xception.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and Validation loss')
plt.legend()

plt.show()

_, acc = model_xception.evaluate_generator(VALIDATION_GENERATOR, steps=10, verbose=0)
print('Test Accuracy: %.3f ' % (acc * 100))

"""### **Prediction**
---
"""

test_x, test_y = VALIDATION_GENERATOR.__getitem__(1)

labels = (VALIDATION_GENERATOR.class_indices)
labels = dict((v,k) for k,v in labels.items())
print(labels)

y_pred = model_xception.predict(test_x)

plt.figure(figsize=(16, 16))
n = 16
count = 0
for i in range(n):
    plt.subplot(4, 4, i+1)
    plt.title('pred:%s / truth:%s' % (labels[np.argmax(y_pred[i])], labels[np.argmax(test_y[i])]))
    plt.imshow(test_x[i])
    if (np.argmax(y_pred[i]) == np.argmax(test_y[i])):
      count += 1

akurasi = count/n*100
print('Accuracy : {} %'.format(akurasi))

"""### **Store Model**
---
"""

!mkdir saved_model_xception
!mkdir tflite_model_xception

"""#### Save Model (h5)
---
"""

saved_model_path = "saved_model_xception/class_detect_best_model.h5"
model_xception.save(saved_model_path)

"""#### Convert to TFLite
---
"""

converter = tf.lite.TFLiteConverter.from_keras_model(model_xception)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model_xception = converter.convert()

import pathlib


tflite_model_file = pathlib.Path('tflite_model_xception/class_detect_best_model.tflite')
tflite_model_file.write_bytes(tflite_model_xception)
print("Model successfully created in .tflite!")

"""## **Save Model to Google Drive**
---
"""

from google.colab import drive
drive.mount("/content/drive")

# Model Inception
source_saved_model_inception = "/content/saved_model_inception"
destination_saved_model_inception = "/content/drive/My Drive/klasifikasi_sampah/class_detect/class_detect_model_inception"
shutil.copytree(source_saved_model_inception, destination_saved_model_inception)

source_tflite_model_inception = "/content/tflite_model_inception"
destination_tflite_model_inception = "/content/drive/My Drive/klasifikasi_sampah/class_detect/class_detect_tflite_model_inception"
shutil.copytree(source_tflite_model_inception, destination_tflite_model_inception)

# Xception
source_saved_model_xception = "/content/saved_model_xception"
destination_saved_model_xception = "/content/drive/My Drive/klasifikasi_sampah/class_detect/class_detect_model_xception"
shutil.copytree(source_saved_model_xception, destination_saved_model_xception)

source_tflite_model_xception = "/content/tflite_model_xception"
destination_tflite_model_xception = "/content/drive/My Drive/klasifikasi_sampah/class_detect/class_detect_tflite_model_xception"
shutil.copytree(source_tflite_model_xception, destination_tflite_model_xception)